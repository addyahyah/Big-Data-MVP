{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "train = pd.read_csv(\"data/announcements.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import various modules for string cleaning\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def review_to_wordlist( review, remove_stopwords=False ):\n",
    "    # Function to convert a document to a sequence of words,\n",
    "    # optionally removing stop words.  Returns a list of words.\n",
    "    #\n",
    "    # 1. Remove HTML\n",
    "    review_text = BeautifulSoup(review).get_text()\n",
    "    #  \n",
    "    # 2. Remove non-letters\n",
    "    review_text = re.sub(\"[^a-zA-Z]\",\" \", review_text)\n",
    "    #\n",
    "    # 3. Convert words to lower case and split them\n",
    "    words = review_text.lower().split()\n",
    "    #\n",
    "    # 4. Optionally remove stop words (false by default)\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "    #\n",
    "    # 5. Return a list of words\n",
    "    return(words)\n",
    "\n",
    "# Download the punkt tokenizer for sentence splitting\n",
    "import nltk.data\n",
    "# nltk.download('punkt')   \n",
    "\n",
    "# Load the punkt tokenizer\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "# Define a function to split a review into parsed sentences\n",
    "def review_to_sentences( review, tokenizer, remove_stopwords=False ):\n",
    "    # Function to split a review into parsed sentences. Returns a \n",
    "    # list of sentences, where each sentence is a list of words\n",
    "    #\n",
    "    # 1. Use the NLTK tokenizer to split the paragraph into sentences\n",
    "    raw_sentences = tokenizer.tokenize(review.strip())\n",
    "    #\n",
    "    # 2. Loop over each sentence\n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        # If a sentence is empty, skip it\n",
    "        if len(raw_sentence) > 0:\n",
    "            # Otherwise, call review_to_wordlist to get a list of words\n",
    "            sentences.append( review_to_wordlist( raw_sentence, \\\n",
    "              remove_stopwords ))\n",
    "    #\n",
    "    # Return the list of sentences (each sentence is a list of words,\n",
    "    # so this returns a list of lists\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing sentences from default set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.5/site-packages/bs4/__init__.py:166: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "To get rid of this warning, change this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n",
      "/opt/anaconda3/lib/python3.5/site-packages/bs4/__init__.py:207: UserWarning: \"b'http://www.bestbuy.com/site/Insignia%26%23153%3B+-+26%26%2334%3B+Class+-+LED+-+1080p+-+60Hz+-+HDTV/5975302.p;jsessionid=4174C8AE162899F3CAC0055456A6E39E.bbolsp-app02-13?id=1218716792876&skuId=5975302&st=insignia%20led%20tv&cp=1&lp=3'\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  '\"%s\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.' % markup)\n",
      "/opt/anaconda3/lib/python3.5/site-packages/bs4/__init__.py:207: UserWarning: \"b'http://www.surveymonkey.com/s/CJW8NMN'\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  '\"%s\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.' % markup)\n",
      "/opt/anaconda3/lib/python3.5/site-packages/bs4/__init__.py:207: UserWarning: \"b'http://www.newegg.com/Product/Product.aspx?Item=N82E16824001385'\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  '\"%s\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.' % markup)\n",
      "/opt/anaconda3/lib/python3.5/site-packages/bs4/__init__.py:207: UserWarning: \"b'http://www.surveymonkey.com/s/TVG8B7G'\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  '\"%s\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.' % markup)\n",
      "/opt/anaconda3/lib/python3.5/site-packages/bs4/__init__.py:207: UserWarning: \"b'https://docs.google.com/spreadsheet/ccc?key=0AnKGlWYzguYldHJ3dDRrYkM1LXdqSXhGRDBZSkZaaXc&usp=sharing'\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  '\"%s\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.' % markup)\n",
      "/opt/anaconda3/lib/python3.5/site-packages/bs4/__init__.py:207: UserWarning: \"b'http://www.indianarailwaymuseum.org/dinosauretrainfrenchlickscenicrailway.html'\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  '\"%s\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.' % markup)\n",
      "/opt/anaconda3/lib/python3.5/site-packages/bs4/__init__.py:207: UserWarning: \"b'http://www.matmartinez.net/nsfw/'\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  '\"%s\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.' % markup)\n",
      "/opt/anaconda3/lib/python3.5/site-packages/bs4/__init__.py:207: UserWarning: \"b'http://images5.fanpop.com/image/photos/25500000/Aww-Yeah-random-25538108-268-209.gif'\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  '\"%s\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.' % markup)\n",
      "/opt/anaconda3/lib/python3.5/site-packages/bs4/__init__.py:207: UserWarning: \"b'https://www.canvas.net/courses/human-element-an-essential-online-course-component-1'\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  '\"%s\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.' % markup)\n",
      "/opt/anaconda3/lib/python3.5/site-packages/bs4/__init__.py:207: UserWarning: \"b'http://www.surveymonkey.com/s/GH9B7FY'\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  '\"%s\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.' % markup)\n",
      "/opt/anaconda3/lib/python3.5/site-packages/bs4/__init__.py:207: UserWarning: \"b'http://universityinnovation.org/wiki/How_to_assess_your_impact_and_the_innovation_ecosystem'\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  '\"%s\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.' % markup)\n",
      "/opt/anaconda3/lib/python3.5/site-packages/bs4/__init__.py:207: UserWarning: \"b'http://terrehaute.craigslist.org/fuo/4181144761.html'\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  '\"%s\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.' % markup)\n",
      "/opt/anaconda3/lib/python3.5/site-packages/bs4/__init__.py:207: UserWarning: \"b'http://terrehaute.craigslist.org/spo/4210173125.html'\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  '\"%s\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.' % markup)\n",
      "/opt/anaconda3/lib/python3.5/site-packages/bs4/__init__.py:207: UserWarning: \"b'http://www.newegg.com/Product/Product.aspx?Item=N82E16842101312'\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  '\"%s\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.' % markup)\n",
      "/opt/anaconda3/lib/python3.5/site-packages/bs4/__init__.py:207: UserWarning: \"b'http://www.dailymail.co.uk/news/article-2577477/Usually-solitary-red-panda-cubs-amaze-zoo-crowd-playful-fight-5ft-prancing-Cincinatti-snow.html'\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  '\"%s\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.' % markup)\n",
      "/opt/anaconda3/lib/python3.5/site-packages/bs4/__init__.py:207: UserWarning: \"b'https://www.surveymonkey.com/s/MNXVDX8\\n\\nThanks!'\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  '\"%s\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.' % markup)\n",
      "/opt/anaconda3/lib/python3.5/site-packages/bs4/__init__.py:207: UserWarning: \"b'https://www.surveymonkey.com/s/D2ZNXXJ'\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  '\"%s\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.' % markup)\n",
      "/opt/anaconda3/lib/python3.5/site-packages/bs4/__init__.py:207: UserWarning: \"b'http://indianapolis.craigslist.org/bik/4433144563.html'\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  '\"%s\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.' % markup)\n",
      "/opt/anaconda3/lib/python3.5/site-packages/bs4/__init__.py:207: UserWarning: \"b'https://www.youtube.com/watch?v=G7alK79yprI'\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  '\"%s\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.' % markup)\n",
      "/opt/anaconda3/lib/python3.5/site-packages/bs4/__init__.py:207: UserWarning: \"b'http://www.midwayusa.com/product/2606114047/tru-spec-gi-spec-3-day-military-backpack'\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  '\"%s\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.' % markup)\n",
      "/opt/anaconda3/lib/python3.5/site-packages/bs4/__init__.py:207: UserWarning: \"b'http://www.rose-hulman.edu/~duffytj/'\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  '\"%s\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.' % markup)\n",
      "/opt/anaconda3/lib/python3.5/site-packages/bs4/__init__.py:207: UserWarning: \"b'http://www.heartlandconf.org/sports/fball/2014-15/play_of_the_week/week_1'\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  '\"%s\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.' % markup)\n",
      "/opt/anaconda3/lib/python3.5/site-packages/bs4/__init__.py:207: UserWarning: \"b'https://docs.google.com/forms/d/1CKSBzwvJGdiNsAI5s-htMNeyqcqvwMcY2LD-Aa4SwkA/viewform?c=0&w=1&usp=mail_form_link\\xe2\\x80\\x8b\\n\\nThanks!'\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  '\"%s\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.' % markup)\n",
      "/opt/anaconda3/lib/python3.5/site-packages/bs4/__init__.py:207: UserWarning: \"b'http://www.newlin-johnson.idxco.com/idx/12301/details.php?idxID=502&listingID=73916'\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  '\"%s\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.' % markup)\n",
      "/opt/anaconda3/lib/python3.5/site-packages/bs4/__init__.py:207: UserWarning: \"b'http://www.newegg.com/Product/Product.aspx?Item=9SIA25V1TV9377&nm_mc=KNC-GoogleMKP-PC&cm_mmc=KNC-GoogleMKP-PC-_-pla-_-Pro++Sound-_-9SIA25V1TV9377\\xe2\\x80\\x8b'\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  '\"%s\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.' % markup)\n",
      "/opt/anaconda3/lib/python3.5/site-packages/bs4/__init__.py:207: UserWarning: \"b'https://docs.google.com/forms/d/1SV513q0eIOlDyVJ-lQqA27m_Nr8uG-pl01PWK7W_414/viewform?usp=send_form\\xe2\\x80\\x8b'\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  '\"%s\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.' % markup)\n"
     ]
    }
   ],
   "source": [
    "sentences = []  # Initialize an empty list of sentences\n",
    "\n",
    "print(\"Parsing sentences from default set\")\n",
    "for review in train[\"Body\"]:\n",
    "    sentences += review_to_sentences(str(review), tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16604\n"
     ]
    }
   ],
   "source": [
    "print(len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['happy', 'new', 'year', 'everyone']\n"
     ]
    }
   ],
   "source": [
    "print (sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n"
     ]
    }
   ],
   "source": [
    "# Import the built-in logging module and configure it so that Word2Vec \n",
    "# creates nice output messages\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\\\n",
    "    level=logging.INFO)\n",
    "\n",
    "# Set values for various parameters\n",
    "num_features = 100    # Word vector dimensionality                      \n",
    "min_word_count = 5   # Minimum word count                        \n",
    "num_workers = 4       # Number of threads to run in parallel\n",
    "context = 10          # Context window size                                                                                    \n",
    "downsampling = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "# Initialize and train the model (this will take some time)\n",
    "from gensim.models import word2vec\n",
    "print (\"Training model...\")\n",
    "model = word2vec.Word2Vec(sentences, workers=num_workers, \\\n",
    "            size=num_features, min_count = min_word_count, \\\n",
    "            window = context, sample = downsampling)\n",
    "\n",
    "# If you don't plan to train the model any further, calling \n",
    "# init_sims will make the model much more memory-efficient.\n",
    "model.init_sims(replace=True)\n",
    "\n",
    "# It can be helpful to create a meaningful model name and \n",
    "# save the model for later use. You can load it later using Word2Vec.load()\n",
    "model_name = \"100features_5minwords_10context_AnnouncementTitle\"\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for K Means clustering:  16.41153621673584 seconds.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import time\n",
    "\n",
    "start = time.time() # Start time\n",
    "\n",
    "# Set \"k\" (num_clusters) to be 1/5th of the vocabulary size, or an\n",
    "# average of 5 words per cluster\n",
    "word_vectors = model.syn0\n",
    "num_clusters = int(word_vectors.shape[0] / 5)\n",
    "\n",
    "# Initalize a k-means object and use it to extract centroids\n",
    "kmeans_clustering = KMeans( n_clusters = num_clusters )\n",
    "idx = kmeans_clustering.fit_predict( word_vectors )\n",
    "\n",
    "# Get the end time and print how long the process took\n",
    "end = time.time()\n",
    "elapsed = end - start\n",
    "print (\"Time taken for K Means clustering: \", elapsed, \"seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a Word / Index dictionary, mapping each vocabulary word to\n",
    "# a cluster number                                                                                            \n",
    "word_centroid_map = dict(zip( model.index2word, idx ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "764"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cluster 0\n",
      "['picture']\n",
      "\n",
      "Cluster 1\n",
      "['consists', 'religion', 'regularly', 'studio', 'country', 'determined', 'community', 'trials', 'anticipated', 'autonomous', 'gender', 'characteristics', 'cross', 'engineer', 'faster', 'art', 'explore', 'activity', 'success', 'apply']\n",
      "\n",
      "Cluster 2\n",
      "['externalclassd', 'baf', 'db', 'ae', 'ce', 'c', 'aac', 'bf', 'aa', 'externalclass', 'cd', 'b', 'ea', 'cf', 'cc', 'bd', 'dd', 'da', 'f', 'ca']\n",
      "\n",
      "Cluster 3\n",
      "['jan', 'plane', 'noon', 'afternoon', 'tonight', 'indi', 'nov', 'evening']\n",
      "\n",
      "Cluster 4\n",
      "['meadowla', 'klineal', 'advance', 'thompsm', 'kepchaja', 'munstebc']\n",
      "\n",
      "Cluster 5\n",
      "['sized', 'boy', 'graphing', 'fridge', 'drawer', 'ex', 'clip', 'originally', 'smartphone', 'sofa', 'jbl', 'enclosure', 'shelves']\n",
      "\n",
      "Cluster 6\n",
      "['no', 'decided', 'blank', 'only']\n",
      "\n",
      "Cluster 7\n",
      "['logic', 'solving', 'norton', 'source', 'sciences', 'meetings', 'youth', 'worlds', 'cyber', 'approach', 'ultimate', 'computational', 'theory', 'analysis', 'mass', 'finance', 'presents', 'managing', 'ghahramani']\n",
      "\n",
      "Cluster 8\n",
      "['rose', 'hulman', 'edu', 'jacksoj']\n",
      "\n",
      "Cluster 9\n",
      "['stay', 'lamps', 'term', 'selected', 'move', 'communicate', 'stuff', 'better', 'fun']\n",
      "\n",
      "Cluster 10\n",
      "['syrian']\n",
      "\n",
      "Cluster 11\n",
      "['angel', 'rosexperience', 'clicking', 'logging', 'visiting']\n",
      "\n",
      "Cluster 12\n",
      "['sets', 'recommend', 'fits', 'hard', 'medium', 'recliners', 'silver', 'bumper', 'dorm']\n",
      "\n",
      "Cluster 13\n",
      "['middleton', 'cto']\n",
      "\n",
      "Cluster 14\n",
      "['money', 'really', 'way', 'staying', 'people']\n",
      "\n",
      "Cluster 15\n",
      "['dark', 'cleaning', 'webcam', 'tan', 'key', 'grey', 'written', 'plate', 'gold']\n",
      "\n",
      "Cluster 16\n",
      "['yard', 'height', 'ground', 'identification', 'pink', 'role', 'removable', 'wide', 'stone', 'fold', 'hot', 'bicycle', 'sitting', 'son', 'driver', 'litter', 'assembly', 'base', 'chloride', 'pull', 'range', 'ceiling', 'ball', 'tape']\n",
      "\n",
      "Cluster 17\n",
      "['jacket']\n",
      "\n",
      "Cluster 18\n",
      "['rd', 'university']\n",
      "\n",
      "Cluster 19\n",
      "['pick', 'up']\n",
      "\n",
      "Cluster 20\n",
      "['lg', 'apple', 'batteries', 'preowned', 'takes', 'unopened']\n",
      "\n",
      "Cluster 21\n",
      "['fbfb', 'dbdda', 'abe', 'ddabdc']\n",
      "\n",
      "Cluster 22\n",
      "['golf', 'strongest', 'dodgeball', 'racquetball', 'scramble', 'track', 'tennis']\n",
      "\n",
      "Cluster 23\n",
      "['statics', 'em', 'thomas', 'textbook', 'bookstore', 'biochemical', 'chem', 'ii', 'calculus', 'ma']\n",
      "\n",
      "Cluster 24\n",
      "['underneath', 'covers', 'noticeable', 'dogs', 'pieces', 'working', 'sturdy', 'carpet', 'owned', 'stand', 'damage', 'rather']\n",
      "\n",
      "Cluster 25\n",
      "['knows', 'let', 'know', 'if']\n",
      "\n",
      "Cluster 26\n",
      "['hoping', 'apartment', 'friend']\n",
      "\n",
      "Cluster 27\n",
      "['mm', 'v', 'series', 'playstation', 'lcd', 'sony', 'watt', 'display']\n",
      "\n",
      "Cluster 28\n",
      "['spend', 'part', 'ss', 'difficult', 'waste', 'clad', 'often', 'allowed', 'attempt', 'separately', 'quick', 'care', 'vacation', 'federal']\n",
      "\n",
      "Cluster 29\n",
      "['algebra', 'ades', 'dfm', 'boundary', 'python', 'philosophy', 'intro', 'mastering', 'retails', 'workbook', 'code']\n",
      "\n",
      "Cluster 30\n",
      "['erin', 'washington', 'reynolds', 'peter', 'mueller', 'matt', 'purdue', 'innovation', 'allen', 'mike', 'taylor', 'johnson', 'nick']\n",
      "\n",
      "Cluster 31\n",
      "['muscle', 'nyquist', 'alpha', 'linear', 'improved', 'assisted', 'magnetic', 'ramp', 'kim', 'uap', 'behavior', 'electronic', 'phase', 'coding', 'cps', 'kid', 'honor', 'synthetic', 'gmos', 'extraction', 'challenges', 'complex', 'skyrim', 'shearer', 'hip']\n",
      "\n",
      "Cluster 32\n",
      "['sw', 'adminsvcs', 'htm', 'users', 'index', 'groups', 'rosie']\n",
      "\n",
      "Cluster 33\n",
      "['tags', 'enough', 'she', 'doesn', 'space', 'sweet', 'isn', 'ever']\n",
      "\n",
      "Cluster 34\n",
      "['enrollment']\n",
      "\n",
      "Cluster 35\n",
      "['negotiate', 'dollars', 'reason']\n",
      "\n",
      "Cluster 36\n",
      "['pack', 'alternative', 'technologies', 'yamaha', 'password', 'weed', 'low', 'dimensions', 'raised', 'connects', 'qualified']\n",
      "\n",
      "Cluster 37\n",
      "['sharing', 'agents']\n",
      "\n",
      "Cluster 38\n",
      "['mees', 'went', 'dropped', 'probably', 'planned', 'gets']\n",
      "\n",
      "Cluster 39\n",
      "['olin', 'moench', 'floor']\n",
      "\n",
      "Cluster 40\n",
      "['tahoma', 'font', 'msochpdefault', 'calibri', 'serif', 'sans', 'family', 'windowtext']\n",
      "\n",
      "Cluster 41\n",
      "['tiftjr', 'khoujayh']\n",
      "\n",
      "Cluster 42\n",
      "['nice', 'works', 'unlocked', 'original']\n",
      "\n",
      "Cluster 43\n",
      "['ages', 'sounds', 'refund', 'pi', 'captain', 'possibility', 'noted', 'request', 'issue', 'iphones', 'sean', 'word', 'jacob', 'carter', 'dollar', 'bic', 'coach']\n",
      "\n",
      "Cluster 44\n",
      "['trace', 'updated', 'face', 'pouches', 'porch', 'scratch', 'technique', 'five', 'stairs', 'popular', 'fireplace', 'wear', 'snow', 'ethernet', 'bags', 'bathroom', 'once', 'clearance', 'dents', 'desks', 'covered', 'camera', 'frames', 'designed']\n",
      "\n",
      "Cluster 45\n",
      "['everyone', 'graduating']\n",
      "\n",
      "Cluster 46\n",
      "['begins', 'nd', 'class']\n",
      "\n",
      "Cluster 47\n",
      "['from', 'friday', 'sunday', 'saturday', 'arrive']\n",
      "\n",
      "Cluster 48\n",
      "['health', 'ups', 'evolution', 'oil', 'associate', 'rate', 'development', 'schultz', 'scale', 'ieee', 'pcs', 'adam', 'language', 'syed', 'conference', 'mary']\n",
      "\n",
      "Cluster 49\n",
      "['pass', 'volunteers', 'remove', 'filled', 'practice', 'dinner', 'hands', 'roughly', 'distance', 'advantage']\n"
     ]
    }
   ],
   "source": [
    "# For the first 10 clusters\n",
    "for cluster in range(0,50):\n",
    "    #\n",
    "    # Print the cluster number  \n",
    "    print (\"\\nCluster %d\" % cluster)\n",
    "    #\n",
    "    # Find all of the words for that cluster number, and print them out\n",
    "    words = []\n",
    "    v=list(word_centroid_map.values())\n",
    "    k = list(word_centroid_map.keys())\n",
    "    for i in range(0,len(word_centroid_map.values())):\n",
    "        if( v[i] == cluster ):\n",
    "            words.append(k[i])\n",
    "    print (words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_bag_of_centroids( wordlist, word_centroid_map ):\n",
    "    #\n",
    "    # The number of clusters is equal to the highest cluster index\n",
    "    # in the word / centroid map\n",
    "    num_centroids = max( word_centroid_map.values() ) + 1\n",
    "    #\n",
    "    # Pre-allocate the bag of centroids vector (for speed)\n",
    "    bag_of_centroids = np.zeros( num_centroids, dtype=\"float32\" )\n",
    "    #\n",
    "    # Loop over the words in the review. If the word is in the vocabulary,\n",
    "    # find which cluster it belongs to, and increment that cluster count \n",
    "    # by one\n",
    "    for word in wordlist:\n",
    "        if word in word_centroid_map:\n",
    "            index = word_centroid_map[word]\n",
    "            bag_of_centroids[index] += 1\n",
    "    #\n",
    "    # Return the \"bag of centroids\"\n",
    "    return bag_of_centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'clean_train_reviews' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-1bed9009488c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Transform the training set reviews into bags of centroids\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mcounter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mreview\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mclean_train_reviews\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0mtrain_centroids\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcounter\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_bag_of_centroids\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mreview\u001b[0m\u001b[1;33m,\u001b[0m         \u001b[0mword_centroid_map\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mcounter\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'clean_train_reviews' is not defined"
     ]
    }
   ],
   "source": [
    "#Pre-allocate an array for the training set bags of centroids (for speed)\n",
    "train_centroids = np.zeros( (train[\"Body\"].size, num_clusters), \\\n",
    "    dtype=\"float32\" )\n",
    "\n",
    "# Transform the training set reviews into bags of centroids\n",
    "counter = 0\n",
    "for review in clean_train_reviews:\n",
    "    train_centroids[counter] = create_bag_of_centroids( review, \\\n",
    "        word_centroid_map )\n",
    "    counter += 1\n",
    "\n",
    "# Repeat for test reviews \n",
    "test_centroids = np.zeros(( train[\"Body\"].size, num_clusters), \\\n",
    "    dtype=\"float32\" )\n",
    "\n",
    "counter = 0\n",
    "for review in clean_test_reviews:\n",
    "    test_centroids[counter] = create_bag_of_centroids( review, \\\n",
    "        word_centroid_map )\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
